# README
## Como instalar e executar o projeto
Não há necessidade de instalação, todo o projeto foi elaborado utilizando o Google Colaboratory, de modo que há um link aqui no projeto que leva ao colab, caso queiram executar os códigos. 
## Bibliotecas utilizadas

- Pandas: manipulação de dados tabulares

- Seaborn: visualização de dados

- Matplotlib: criar gráficos

- Módulo 're': trabalhar com expressões regulares

- NumPy: suporte a arrays e operações matemáticas

- SciPy: funções estatísticas

- Joblib: salvar e carregar objetos Python

- SimpleImputer: preenchimento de valores ausentes

- train_test_split: dividir o conjunto de dados em treinamento e teste

- RandomForestRegressor: regressão usando o algoritmo Random Forest

- r2_score: avaliação de modelos de regressão

Quanto às versões, utilizei as já instaladas no Google Colaboratory
## Relatórios - EDA e Análises Estatísticas, Códigos de modelagem e Entrega de arquivo final (predicted.csv)
Os relatórios e todos os códigos utilizados estão dentro desse notebook: [LH_CD_Alvaro_Martins_Alves.ipynb](https://github.com/AlvaroMAlves/LH_CD_Alvaro_Martins_Alves/blob/main/LH_CD_Alvaro_Martins_Alves.ipynb).

O arquivo *predicted.csv* está no repositório.


## Observação
Ao utilizar o modelo final no aqruivo *cars_test.csv*, tive de retirar e adicionar algumas colunas, já que utilizei a marca e a cor como variáveis de predição, e nem todas estavam presentes em ambos os arquivos.
